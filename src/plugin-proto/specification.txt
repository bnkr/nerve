TODO
----

* implement a working section class.  See the notes in the "section" part of the
  spec-bits.cpp.
  - decide where the connectors are going to go
  - whether we pass the packets directly or not (requires the "debuffer" method
    on sequences)
  - just hack something up for now.  It doesn't need  to be ultimate, just so I
    can begin to move towards making it actually compile.  I have a hard time
    believing I can really remove the extra if statements everywhere because
    they're simply a consequence of the requirement.

The Pipeline
------------

The player is an ordered list of 'stages' which form a 'pipeline'.  All
groupings of 'stages' remain ordered.

The 'pipeline' is also the order of inter-stage communication of 'packets'.
'packets' are the data which is being processed.  'Events' are types of 'packet'
on the 'pipeline' which do not carry processable data.

A visit to a 'stage' in an iteration of a 'pipeline' is called a 'stage
operation'; 'stage operations' always involve giving a 'packet' to the stage for
handling.  [*note:* This handling may be implicit; for example a particular
'event' might be converted into a method call.]

Inter-'stage' communication happens down "first in first out" structures called
'pipes'.  A 'pipe' can be a 'thread pipe' which enables communication across
multiple threads, or a 'local pipe' which works within the same thread.

Communication between 'stages' must respect the 'constant delay rule'.  This is
defined as such: given an single iteration of a 'pipeline' of one or more
'stages', no 'stage' must be visited twice.

[*note:* This is a tricky concept.  It is important when 'stages' can output
multiple 'packets' to preserve latency to the next 'stage'.  If the delay
changes then part of the 'pipeline' will starve for input.

For example, consider three connected 'stages' A, B, and C.  Each 'stage'
outputs its input 'packet' twice.  A trivial 'packet' passing algorithm would
buffer the outputs from a 'stage' and then iterate them to call the next
'stage'.  This results in stage A running once, then stage B running twice, and
stage C four times.  Now consider what would happen if each stage would produce
variable amounts of outputs.  The number of 'stage' operations from A to C
becomes variable.]

Pipeline Hierarchy
------------------

Similarly typed 'stages' are grouped into 'stage sequences'.  A 'stage sequence'
is a mono-threaded, mono-stage typed group.  [*note:* Mono-typing is necessary
to ensure efficiency given the multitude of differing operations on different
'stages'.]

'Stage sequences' are grouped into 'sections'.  A 'section' is a mono-threaded,
multi-'stage' typed group (via polymorphic 'sequence' types).  'Section' always
communicate with other threads.  [*note:* 'Thread pipes' also trivially enforce
the 'constant delay rule' between connected 'stages'.]

'Sections' are grouped in 'jobs'.  'Jobs' map exactly onto threads.

A 'stage sequence's' loop through its contained list of 'stages' is called a
'sequence step' [*note:* Not an "iteration" because it doesn't necessarily visit
all 'stages' and it only does one pass while the full iteration of the program
is infinite.]  A single iterations of 'stage sequences' contained in a 'section'
is called a 'section step'.

'Stages' which are directly next to each other in 'pipeline' order are said to
be 'connected'.  The same is said of 'sequences' and 'sections'.

'Stages' are stored in 'pipeline' order.  [*note:* This extends to indirect
storage by way of a 'sequence' or 'section'.]

Stages
------

There are several types of 'stage operation'.

* 'process' -- specified below.
* 'debuffer' -- specified below.
* 'observe' -- an input is given, but no output is possible ('packets' always
  propagate).
* 'flush' -- consider any stateful computation to be finished and begin
  de-buffering any data which is buffered (e.g. play list finished)
* 'finish' -- shutting down.
* 'abandon' -- any data buffered or computations in progress should be discarded
  in preparation for new data (e.g. skip to new song).

A 'stage' which has the 'process' operation is called a 'processor', while one
which has the 'observe' operation is called an 'observer'.  No 'stage' supports
both.  Only 'processors' support the 'debuffer' operation.  Both 'processors' and
'observers' support 'flush' and 'abandon'.

[*note:* Some specialised 'stages' have more operations listed later in this
specification.]

The 'process', 'debuffer', and 'observe' operations are known as 'data
operations'.

Each 'stage operation' as a corresponding 'event packet'.  [*note:*
Corresponding 'events' does not mean that the 'stage' itself would necessarily
decide what 'operation' to run.]

Process and Debuffer
--------------------

The 'process operation' expected a single input 'packet' and one or less output
'packets'.  [*note:* "One or less" preserves 'constant delay'.  If a 'stage
operation' returns lots of packets, the generation of those 'packets' will make
the 'stage operation' longer, even if we progressively buffer the outputs.
Technically, there's no way to forbid a 'stage' from generating loads of
'packets' -- we just don't need to help them do it!] The operation can cause any
number of 'packets' to be generated, delayed, discarded, or modified.

[*note:* It is possible for a 'processor' to to modify or drop 'non-data events'
as it would 'data events'.  In reality though most types of 'non-data events'
would be propagated to the next 'stage' by 'stage' containers without asking the
'stage' (in practical terms: the 'event' results in a void method call on the
'stage'.]

When multiple 'packets' must be outputted, the 'processor stage' must handle
this with internal buffering.  [*note:* The "buffer" is not necessarily a queue.
It may merely indicate some algorithmic state which results in new 'packets'
being created, however, 'stages' which delay 'packets' will naturally need some
kind of tangible buffer.]

A stage which is returning buffered 'packets' is called a 'buffering stage'.  A
'sequence step' containing a 'buffering stage' will start at that 'stage'.  The
'debuffer operation' will be run on the 'buffering stage' in subsequent 'stage
iterations' until the supply of buffered 'packets' is exhausted. [*note:*  No
new 'packets' are introduced so the buffer gets smaller, and no repeated visits
are made so we keep 'constant delay'.]

[*note:* It is preferred to specify that  the 'debuffer' operation is run on the
last 'stage' in the 'sequence' which has buffered data, but it is difficult to
implement.]

The 'debuffer operation' produces one or less 'packets'.  [*note:* Zero output
is allowed because the buffer must empty at some point and the iteration must
realise this.]

No 'debuffer' operation must delay checking the containing 'section's' input
'pipe'.  [*note:* This is necessary to reduce latency 'constant delay'; a naive
implementation might not terminate its iteration until a 'buffering stage' is
finished.  This could take a long time and would be wasted if there was an
'abandon' (or similar) on the 'pipe'.]

Special Stages
--------------

The 'input stage' and 'output stage' are special.  The 'input stage' resides at
the start of the 'pipeline' and is followed by 'processors'.  The 'output stage'
follows the 'processors' and is a kind of specialised 'observer'.  Only
'observers' follow the 'output stage'.

The 'input stage' supports the following 'stage operations':

* 'load' -- load a new stream.
* 'skip' -- skip to a position in the current stream.
* 'pause' -- stop loading information.
* 'data' -- output data from the stream.

'Operations' called on the 'input stage' are translated into 'events'
appropriate for the rest of the 'pipeline'.

The 'output stage' is intended to support writing processed data to sound
hardware.  The 'output stage' has an additional 'reconfigure' operation which
would cause the sound configuration to be modified for different sound data
parameters (such as sample rate).

[*note:* It could be the case that we pass a 'reconfigure event' down the pipe
and then either a sample rate changing 'stage' would deal with it and drop the
'packet' or it would reach the sound card and cause a hardware change.]

The 'output stage' writes data to the 'input stage' to post 'events' which
request more data.  This communication would use a 'pipe' scope ('local' or
'thread') depending on whether  the 'input stage' is in the same 'job' or not.)

Jobs
----

A 'job' must not block multiple times.  [*note:* Consider 'sections' A, B, and C
which are 'connected'.  A and C are in 'job' alpha while B is in job beta.  A
simple 'job' implementation causes a block on each multi-threaded 'connection'
in its iteration.  'Section' A results in an outputted 'packet' to 'section' B.
'Job' alpha will move to 'section' C and wait for input.  'Section' B consumes
the 'packet' originally outputted by 'section' A and waits for more input from
A.  At this point, 'job' alpha is in 'section' C and waiting for 'job' beta.
'Job' beta is in 'section' B and waiting for 'job' alpha.  This is now a
deadlock.]

Problems
--------

.output chunking
* every output will need chunking
* why not make chunking a stage?
* it would mean that the chunks become thread-shared data
* this can't be done at the moment because the final terminator will free them
  whether the stage is done with them or not.
* this is a general problem really, but it could cause particular ire with the
  output situation as mentioned
